{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5: Juan Arroyo. Collaborators: Hannes Koenig, Mark Vandergon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# superclass of neural network \"modules\" (layers)\n",
    "class Module:\n",
    "    \"\"\"\n",
    "    Module is a super class. It could be a single layer, or a multilayer perceptron.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.train = True\n",
    "        return\n",
    "    \n",
    "    def forward(self, _input):\n",
    "        \"\"\"\n",
    "        z = f(a); a is the input, and h is the output.\n",
    "        \n",
    "        Inputs:\n",
    "        _input: a\n",
    "        \n",
    "        Returns:\n",
    "        output z\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def backward(self, _input, _gradOutput):\n",
    "        \"\"\"\n",
    "        Compute:\n",
    "        gradient w.r.t. _input\n",
    "        gradient w.r.t. trainable parameters\n",
    "        \n",
    "        Inputs (in lecture notation):\n",
    "        _input: a \n",
    "        _gradOutput: dL/dz\n",
    "        \n",
    "        Returns:\n",
    "        gradInput: dL/dz\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Return the value of trainable parameters and its corresponding gradient (Used for grandient descent)\n",
    "        \n",
    "        Returns:\n",
    "        params, gradParams\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def training(self):\n",
    "        \"\"\"\n",
    "        Turn the module into training mode.(Only useful for Dropout layer)\n",
    "        Ignore it if you are not using Dropout.\n",
    "        \"\"\"\n",
    "        self.train = True\n",
    "        \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Turn the module into evaluate mode.(Only useful for Dropout layer)\n",
    "        Ignore it if you are not using Dropout.\n",
    "        \"\"\"\n",
    "        self.train = False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a class representing a sequence of modules (a layered network)\n",
    "class Sequential(Module):\n",
    "    \"\"\"\n",
    "    Sequential provides a way to plug layers together in a feed-forward manner.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        Module.__init__(self)\n",
    "        self.layers = [] # layers contain all the layers in order\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer) # Add another layer at the end\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.layers) # How many layers.\n",
    "    \n",
    "    def forward(self, _input):\n",
    "        \"\"\"\n",
    "        Feed forward through all the layers, and return the output of the last layer\n",
    "        \"\"\"\n",
    "        # self._inputs saves the input of each layer\n",
    "        # self._inputs[i] is the input of i-th layer\n",
    "        self._inputs = [_input]\n",
    "        for i in range(self.size()):\n",
    "            # The output of (i-1)-th layer as the _input of i-th layer\n",
    "            self._inputs.append(self.layers[i].forward(self._inputs[i]))\n",
    "        # The last element of self._inputs is the output of last layer\n",
    "        self._output = self._inputs[-1]\n",
    "        return self._output\n",
    "    \n",
    "    def backward(self, _input, _gradOutput):\n",
    "        \"\"\"\n",
    "        Backpropogate through all the layers using chain rule.\n",
    "        \"\"\"\n",
    "        # self._gradInputs[i] is the gradient of loss w.r.t. the input of i-th layer\n",
    "        self._gradInputs = [None] * (self.size() + 1)\n",
    "        self._gradInputs[self.size()] = _gradOutput\n",
    "        for i in reversed(range(self.size())):\n",
    "            self._gradInputs[i] = \\\n",
    "                self.layers[i].backward(self._inputs[i], self._gradInputs[i + 1])\n",
    "        self._gradInput = self._gradInputs[0]\n",
    "        return self._gradInput\n",
    "    \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Return trainable parameters and its corresponding gradient in a nested list\n",
    "        \"\"\"\n",
    "        params = []\n",
    "        gradParams = []\n",
    "        for m in self.layers:\n",
    "            _p, _g = m.parameters()\n",
    "            if _p is not None:\n",
    "                params.append(_p)\n",
    "                gradParams.append(_g)\n",
    "        return params, gradParams\n",
    "\n",
    "    def training(self):\n",
    "        \"\"\"\n",
    "        Turn all the layers into training mode\n",
    "        \"\"\"\n",
    "        Module.training(self)\n",
    "        for m in self.layers:\n",
    "            m.training()\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Turn all the layers into evaluate mode\n",
    "        \"\"\"\n",
    "        Module.evaluate(self)\n",
    "        for m in self.layers:\n",
    "            m.evaluate()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FullyConnected(Module):\n",
    "    \"\"\"\n",
    "    Fully connected layer (parameters include a matrix of weights a vector of biases)\n",
    "    \"\"\"\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        Module.__init__(self)\n",
    "        # Initalization\n",
    "        stdv = 1./np.sqrt(inputSize)\n",
    "        \n",
    "        self.weight = np.random.uniform(-stdv, stdv, (inputSize, outputSize))\n",
    "        self.gradWeight = np.ndarray((inputSize, outputSize))\n",
    "        print(self.gradWeight.shape, \"Weight initial random values\")\n",
    "        self.bias = np.random.uniform(-stdv, stdv, outputSize)\n",
    "        self.gradBias = np.ndarray(outputSize)\n",
    "        \n",
    "    def forward(self, _input):\n",
    "        \"\"\"\n",
    "        output = W * input + b\n",
    "        \"\"\"\n",
    "        self._input = _input\n",
    "        self._output = np.dot(_input, self.weight) + self.bias\n",
    "        return self._output\n",
    "    \n",
    "    def backward(self, _input, _gradOutput):\n",
    "        \"\"\"\n",
    "        gradWeight = gradOutput * input\n",
    "        gradBias = \n",
    "        gradInput =  gradWeight * gradOutput\n",
    "        \"\"\"\n",
    "        self.gradWeight.fill(0)\n",
    "        self.gradBias.fill(0)\n",
    "        \n",
    "        self.gradWeight += _input.T.dot(_gradOutput)\n",
    "        self.gradBias += np.sum(_gradOutput,axis=0)\n",
    "        self._gradInput = _gradOutput.dot(self.weight.T)\n",
    "        return self._gradInput\n",
    "        \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Return weight and bias and their g\n",
    "        \"\"\"\n",
    "        return [self.weight, self.bias], [self.gradWeight, self.gradBias]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    \"\"\"\n",
    "    ReLU activation, not trainable.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        Module.__init__(self)\n",
    "        return\n",
    "    \n",
    "    def forward(self, _input):\n",
    "        \"\"\"\n",
    "        output = max(0, input)\n",
    "        \"\"\"\n",
    "        self._input = _input\n",
    "        self._output =  np.maximum(0, self._input)\n",
    "        return self._output\n",
    "    \n",
    "    def backward(self, _input, _gradOutput):\n",
    "        \"\"\"\n",
    "        gradInput = gradOutput * mask\n",
    "        mask = _input > 0\n",
    "        \"\"\"\n",
    "        self._gradInput = _gradOutput * (self._input > 0)\n",
    "        return self._gradInput\n",
    "        \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        No trainable parametersm, return None\n",
    "        \"\"\"\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Module):\n",
    "    \"\"\"\n",
    "    sigmoid activation, not trainable.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        Module.__init__(self)\n",
    "        return\n",
    "    \n",
    "    def forward(self, _input):\n",
    "        \"\"\"\n",
    "        output = max(0, input)\n",
    "        \"\"\"\n",
    "        self._input = _input\n",
    "        self._output = 1. /  (1 + np.exp(-self._input))\n",
    "        return self._output\n",
    "    \n",
    "    def backward(self, _input, _gradOutput):\n",
    "        \"\"\"\n",
    "        gradInput = gradOutput * mask\n",
    "        mask = _input > 0\n",
    "        \"\"\"\n",
    "        self._gradInput = _gradOutput * (1. - self._output) * self._output\n",
    "        return self._gradInput\n",
    "        \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        No trainable parametersm, return None\n",
    "        \"\"\"\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optional\n",
    "class Dropout(Module):\n",
    "    \"\"\"\n",
    "    A dropout layer\n",
    "    \"\"\"\n",
    "    def __init__(self, p = 0.5):\n",
    "        Module.__init__(self)\n",
    "        self.p = p #self.p is the drop rate, if self.p is 0, then it's a identity layer\n",
    "        \n",
    "    def forward(self, _input):\n",
    "        self._output = _input\n",
    "        if self.p > 0:\n",
    "            if self.train:\n",
    "                # Randomize a mask from bernoulli distrubition\n",
    "                self.mask = np.random.binomial(1, 1 - self.p, _input.shape).astype('float64')\n",
    "                # Scale the mask\n",
    "                self.mask /= 1 - self.p\n",
    "                self._output *= self.mask\n",
    "        return self._output\n",
    "    \n",
    "    def backward(self, _input, _gradOutput):\n",
    "        self._gradInput = _gradOutput\n",
    "        if self.train:\n",
    "            if self.p > 0:\n",
    "                self._gradInput *= self.mask\n",
    "        return self._gradInput\n",
    "    \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        No trainable parameters.\n",
    "        \"\"\"\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SoftMaxLoss(object):\n",
    "    def __init__(self):\n",
    "        return\n",
    "        \n",
    "    def forward(self, _input, _label):\n",
    "        \"\"\"\n",
    "        Softmax and cross entropy loss layer. Should return a scalar, since it's a\n",
    "        loss. (It's almost identical to what we had in Pset 2)\n",
    "\n",
    "        Inputs:\n",
    "        _input: N x C\n",
    "        _labels: N x C, one-hot\n",
    "\n",
    "        Returns: loss (scalar)\n",
    "        \"\"\"\n",
    "        self._input = _input - _input.max(1)[:, np.newaxis]\n",
    "        self._logprob = self._input - np.log(np.exp(self._input).sum(1)[:, np.newaxis])\n",
    "        self._output = np.mean(np.sum(-self._logprob * _label, 1))\n",
    "        return self._output\n",
    "    \n",
    "    def backward(self, _input, _label):\n",
    "        self._gradInput = (np.exp(self._logprob) - _label) / _label.shape[0]\n",
    "        return self._gradInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.31738759809\n",
      "my grad [[ 0.02987379 -0.29002301  0.01666303  0.03417416  0.04503975  0.03570464\n",
      "   0.03440032  0.02966941  0.02641884  0.03807907]\n",
      " [ 0.02959471  0.0404012  -0.3086131   0.02108355  0.03496388  0.04611156\n",
      "   0.03115231  0.02477963  0.04192224  0.03860401]\n",
      " [ 0.03208438  0.03323847  0.0210828  -0.3002426   0.03050131  0.03979036\n",
      "   0.0361958   0.04716939  0.04092929  0.0192508 ]]\n",
      "numerical [[ 0.02987379 -0.29002301  0.01666303  0.03417416  0.04503975  0.03570464\n",
      "   0.03440032  0.02966941  0.02641884  0.03807907]\n",
      " [ 0.02959471  0.0404012  -0.3086131   0.02108355  0.03496388  0.04611156\n",
      "   0.03115231  0.02477963  0.04192224  0.03860401]\n",
      " [ 0.03208438  0.03323847  0.0210828  -0.3002426   0.03050131  0.03979036\n",
      "   0.0361958   0.04716939  0.04092929  0.0192508 ]]\n",
      "7.93413402274e-09\n",
      "error 7.93413402274e-09\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Test softmaxloss, the relative error should be small enough\n",
    "def test_sm():\n",
    "    crit = SoftMaxLoss()\n",
    "    gt = np.zeros((3, 10))\n",
    "    gt[np.arange(3), np.array([1,2,3])] = 1\n",
    "    x = np.random.random((3,10))\n",
    "    def test_f(x):\n",
    "        return crit.forward(x, gt)\n",
    "\n",
    "    print(crit.forward(x, gt))\n",
    "\n",
    "    gradInput = crit.backward(x, gt)\n",
    "    gradInput_num = numeric_gradient(test_f, x, 1, 1e-6)\n",
    "    print(\"my grad\", gradInput)\n",
    "    print(\"numerical\", gradInput_num)\n",
    "    print(relative_error(gradInput, gradInput_num, 1e-8))\n",
    "    error = relative_error(gradInput, gradInput_num, 1e-8)\n",
    "    print(\"error\", error)\n",
    "    print(error <=1e-6)\n",
    "    \n",
    "test_sm()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10) Weight initial random values\n",
      "testing FullyConnected\n",
      "3.81634373313e-09\n",
      "testing ReLU\n",
      "7.36022825478e-10\n",
      "testing dropout\n",
      "7.36022825478e-10\n",
      "(10, 10) Weight initial random values\n",
      "(10, 10) Weight initial random values\n",
      "testing 2-layer model\n",
      "2.97047503507e-08\n"
     ]
    }
   ],
   "source": [
    "# Test modules, all the relative errors should be small enough (on the order of 1e-6 or smaller)\n",
    "def test_module(model):\n",
    "\n",
    "    model.evaluate()\n",
    "\n",
    "    crit = TestCriterion()\n",
    "    gt = np.random.random((3,10))\n",
    "    x = np.random.random((3,10))\n",
    "    def test_f(x):\n",
    "        return crit.forward(model.forward(x), gt)\n",
    "\n",
    "    gradInput = model.backward(x, crit.backward(model.forward(x), gt))\n",
    "    gradInput_num = numeric_gradient(test_f, x, 1, 1e-6)\n",
    "    print(relative_error(gradInput, gradInput_num, 1e-8))\n",
    "\n",
    "# Test fully connected\n",
    "model = FullyConnected(10, 10)\n",
    "print('testing FullyConnected')\n",
    "test_module(model)\n",
    "\n",
    "# Test ReLU\n",
    "model = ReLU()\n",
    "print('testing ReLU')\n",
    "test_module(model)\n",
    "\n",
    "# Test Dropout\n",
    "model = Dropout()\n",
    "print(\"testing dropout\")\n",
    "test_module(model)\n",
    "\n",
    "# Test Sequential\n",
    "model = Sequential()\n",
    "model.add(FullyConnected(10, 10))\n",
    "model.add(ReLU())\n",
    "model.add(FullyConnected(10, 10))\n",
    "#model.add(Dropout())\n",
    "print('testing 2-layer model')\n",
    "test_module(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3) Weight initial random values\n",
      "(3, 1) Weight initial random values\n",
      "0.530324502841\n",
      "0.00967549715902\n",
      "0.00967549715902\n",
      "0.00967549715902\n",
      "0.00967549715902\n",
      "0.00967549715902\n",
      "0.00967549715902\n",
      "0.00967549715902\n",
      "0.00967549715902\n",
      "0.00967549715902\n",
      "0.00967549715902\n"
     ]
    }
   ],
   "source": [
    "# Test gradient descent, the loss should be lower and lower\n",
    "trainX = np.random.random((10,5))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(FullyConnected(5, 3))\n",
    "model.add(ReLU())\n",
    "model.add(Dropout())\n",
    "model.add(FullyConnected(3, 1))\n",
    "\n",
    "crit = TestCriterion()\n",
    "\n",
    "params, gradParams = model.parameters()\n",
    "\n",
    "it = 0\n",
    "state = None\n",
    "while True:\n",
    "    output = model.forward(trainX)\n",
    "    loss = crit.forward(output, None)\n",
    "    if it % 100 == 0:\n",
    "        print(loss)\n",
    "    doutput = crit.backward(output, None)\n",
    "    model.backward(trainX, doutput)\n",
    "    sgdm(params, gradParams, 0.01, 0.8, state)\n",
    "    if it > 1000:\n",
    "        break\n",
    "    it += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start to work on real data. The first one is Fashion MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load large trainset.\n",
      "(50000, 784)\n",
      "(50000, 10)\n",
      "Load valset.\n",
      "(10000, 784)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import FMNIST_utils\n",
    "\n",
    "# We only consider large set this time\n",
    "print(\"Load large trainset.\")\n",
    "Xlarge,Ylarge = FMNIST_utils.load_data(\"Tr\")\n",
    "print(Xlarge.shape)\n",
    "print(Ylarge.shape)\n",
    "\n",
    "print(\"Load valset.\")\n",
    "Xval,Yval = FMNIST_utils.load_data(\"Vl\")\n",
    "print(Xval.shape)\n",
    "print(Yval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X, model):\n",
    "    \"\"\"\n",
    "    Evaluate the soft predictions of the model.\n",
    "    Input:\n",
    "    X : N x d array (no unit terms)\n",
    "    model : a multi-layer perceptron\n",
    "    Output:\n",
    "    yhat : N x C array\n",
    "        yhat[n][:] contains the score over C classes for X[n][:]\n",
    "    \"\"\"\n",
    "    return model.forward(X)\n",
    "\n",
    "def error_rate(X, Y, model):\n",
    "    \"\"\"\n",
    "    Compute error rate (between 0 and 1) for the model\n",
    "    \"\"\"\n",
    "    model.evaluate()\n",
    "    res = 1 - (model.forward(X).argmax(-1) == Y.argmax(-1)).mean()\n",
    "    model.training()\n",
    "    return res\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "def runTrainVal(X,Y,model,Xval,Yval,trainopt):\n",
    "    \"\"\"\n",
    "    Run the train + evaluation on a given train/val partition\n",
    "    trainopt: various (hyper)parameters of the training procedure\n",
    "    During training, choose the model with the lowest validation error. (early stopping)\n",
    "    Assumes (global) variable crit containing the loss (training \"criterion\" to be minimized)\n",
    "    \"\"\"\n",
    "    \n",
    "    params, gradParams = model.parameters()\n",
    "    \n",
    "    eta = trainopt['eta']\n",
    "    \n",
    "    N = X.shape[0] # number of data points in X\n",
    "    \n",
    "    # Save the model with lowest validation error\n",
    "    minValError = np.inf\n",
    "    saved_model = None # Save the best model accoring to validation error\n",
    "    \n",
    "    shuffled_idx = np.random.permutation(N)\n",
    "    start_idx = 0\n",
    "    for iteration in range(trainopt['maxiter']):\n",
    "        if iteration % int(trainopt['eta_frac'] * trainopt['maxiter']) == 0:\n",
    "            eta *= trainopt['etadrop']\n",
    "        # form the next mini-batch\n",
    "        stop_idx = min(start_idx + trainopt['batch_size'], N)\n",
    "        batch_idx = range(N)[int(start_idx):int(stop_idx)]\n",
    "        \n",
    "        s_idx = shuffled_idx[batch_idx]\n",
    "        \n",
    "        bX = X[s_idx,:]\n",
    "        bY = Y[s_idx,:]\n",
    "\n",
    "        score = model.forward(bX)\n",
    "        loss = crit.forward(score, bY)\n",
    "        # note: this computes loss on the *batch* only, not on the entire training set!\n",
    "        \n",
    "        dscore = crit.backward(score, bY)\n",
    "        model.backward(bX, dscore)\n",
    "        \n",
    "        # Update the data using preferred update rule\n",
    "        \n",
    "        if trainopt['update'] == 'sgdm':\n",
    "            sgdm(params, gradParams, eta, weight_decay = trainopt['lambda'])    \n",
    "        elif trainopt['update'] == 'sgd':\n",
    "            sgd(params, gradParams, eta, weight_decay = trainopt['lambda'])\n",
    "        elif trainopt['update'] == 'nesterov':\n",
    "            sgdmom(params, gradParams, eta, weight_decay = trainopt['lambda'])\n",
    "\n",
    "\n",
    "        start_idx = stop_idx % N\n",
    "        \n",
    "        if (iteration % trainopt['display_iter']) == 0:\n",
    "            #compute train and val error; multiply by 100 for readability (make it percentage points)\n",
    "            trainError = 100 * error_rate(X, Y, model)\n",
    "            valError = 100 * error_rate(Xval, Yval, model)\n",
    "            print('{:8} batch loss: {:.3f} train error: {:.3f} val error: {:.3f}'.format(iteration, loss, trainError, valError))\n",
    "            \n",
    "            # early stopping: save the best model snapshot so far (i.e., model with lowest val error)\n",
    "            if valError < minValError:\n",
    "                saved_model = deepcopy(model)\n",
    "                minValError = valError\n",
    "        \n",
    "    return saved_model, minValError, trainError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(input_size, hidden_size, output_size, activation_func = 'ReLU', dropout = 0):\n",
    "    \"\"\"\n",
    "    Build a model:\n",
    "    input_size: the dimension of input data\n",
    "    hidden_size: the dimension of hidden vector, hidden_size == 0 means only one layer;\n",
    "        hidden_size = [h1, h2, ...] specifies multiple layers of sizes h1, h2 etc.\n",
    "    output_size: the output size of final layer (typically, number of classes).\n",
    "    activation_func: ReLU, sigmoid (defined above), Tanh (you'd have to define), etc. \n",
    "    dropout: the dropout rate: if dropout == 0, this is equivalent to no dropout\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    if type(hidden_size) is int:\n",
    "        hidden_size = [hidden_size] # ensure it's a list\n",
    "    \n",
    "    prev_size=input_size\n",
    "    \n",
    "    # add hidden layer(s) as requested\n",
    "    if hidden_size[0] == 0: # no hidden layer\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        for l in range(len(hidden_size)):\n",
    "            model.add(FullyConnected(prev_size, hidden_size[l]))\n",
    "            prev_size=hidden_size[l]\n",
    "            \n",
    "            if activation_func == 'ReLU':\n",
    "                model.add(ReLU())\n",
    "            elif activation_func == 'sigmoid':\n",
    "                model.add(Sigmoid())\n",
    "                \n",
    "            if dropout > 0:\n",
    "                model.add(Dropout(dropout))\n",
    "                \n",
    "    # now add output layer  \n",
    "    model.add(FullyConnected(prev_size, output_size))\n",
    "\n",
    "        \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of how one can define, train and evaluate a model (in this case a three-layer model, with 200 units in each hidden layer, for Fashion MNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Training: 200 units per layer, 5 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 200) Weight initial random values\n",
      "(200, 200) Weight initial random values\n",
      "(200, 200) Weight initial random values\n",
      "(200, 200) Weight initial random values\n",
      "(200, 200) Weight initial random values\n",
      "(200, 10) Weight initial random values\n",
      "       0 batch loss: 58.513 train error: 88.830 val error: 88.690\n",
      "    5000 batch loss: 2.185 train error: 81.898 val error: 81.540\n",
      "   10000 batch loss: 2.235 train error: 83.212 val error: 82.780\n",
      "   15000 batch loss: 1.928 train error: 86.564 val error: 86.030\n",
      "   20000 batch loss: 2.119 train error: 87.650 val error: 87.070\n",
      "   25000 batch loss: 2.138 train error: 87.242 val error: 86.750\n",
      "   30000 batch loss: 1.905 train error: 82.124 val error: 81.880\n",
      "   35000 batch loss: 1.919 train error: 82.384 val error: 82.180\n",
      "   40000 batch loss: 2.060 train error: 83.574 val error: 83.060\n",
      "   45000 batch loss: 2.079 train error: 85.518 val error: 85.100\n",
      "   50000 batch loss: 2.038 train error: 86.454 val error: 85.850\n",
      "   55000 batch loss: 1.950 train error: 87.360 val error: 86.880\n",
      "train set model [ h = 200 200 200 200 200  ], lambda= 0.0001 ] --> train error: 87.36, val error: 81.54\n"
     ]
    }
   ],
   "source": [
    "trainopt = {\n",
    "    'eta': 1e-3,   # initial learning rate\n",
    "    'maxiter': 60000,   # max number of iterations (updates) of SGD\n",
    "    'display_iter': 5000,  # display batch loss every display_iter updates\n",
    "    'batch_size': 128,  \n",
    "    'etadrop': .5, # when dropping eta, multiply it by this number (e.g., .5 means halve it)\n",
    "    'eta_frac': .25,  # drop eta after every eta_frac*maxiter\n",
    "    'update': 'sgdm' # SGD with momentum (using the default momentum value, see utils.py)\n",
    "}\n",
    "\n",
    "NFEATURES = Xlarge.shape[1]\n",
    "\n",
    "# we will maintain a record of models trained for different values of lambda\n",
    "# these will be indexed directly by lambda value itself\n",
    "trained_models = dict()\n",
    "\n",
    "# set the (initial?) set of hyperparameters to explore\n",
    "\n",
    "lambda_=0.0001\n",
    "hidden_size_=[200,200,200,200,200] # five hidden layers, 200 units each\n",
    "\n",
    "trainopt['lambda'] = lambda_\n",
    "model = build_model(NFEATURES, hidden_size_, 10, dropout = 0.8)\n",
    "crit = SoftMaxLoss()\n",
    "# -- model trained on large train set\n",
    "trained_model,valErr,trainErr = runTrainVal(Xlarge, Ylarge, model, Xval, Yval, trainopt)\n",
    "trained_models[lambda_] = {'model': trained_model, \"val_err\": valErr, \"train_err\": trainErr }\n",
    "print('train set model [ h = ',end='')\n",
    "for l in range(len(hidden_size_)):\n",
    "    print('%d '%hidden_size_[l],end='')\n",
    "print(' ], lambda= %.4f ] --> train error: %.2f, val error: %.2f' % (lambda_, trainErr, valErr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Training: 1000 units per layer,  2 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 1000) Weight initial random values\n",
      "(1000, 1000) Weight initial random values\n",
      "(1000, 10) Weight initial random values\n",
      "       0 batch loss: 86.717 train error: 78.430 val error: 78.670\n",
      "    5000 batch loss: 1.020 train error: 22.252 val error: 23.670\n",
      "   10000 batch loss: 0.801 train error: 19.712 val error: 21.310\n"
     ]
    }
   ],
   "source": [
    "trainopt = {\n",
    "    'eta': 1e-3,   # initial learning rate\n",
    "    'maxiter': 60000,   # max number of iterations (updates) of SGD\n",
    "    'display_iter': 5000,  # display batch loss every display_iter updates\n",
    "    'batch_size': 128,  \n",
    "    'etadrop': .5, # when dropping eta, multiply it by this number (e.g., .5 means halve it)\n",
    "    'eta_frac': .25,  # drop eta after every eta_frac*maxiter\n",
    "    'update': 'sgdm' # SGD with momentum (using the default momentum value, see utils.py)\n",
    "}\n",
    "\n",
    "NFEATURES = Xlarge.shape[1]\n",
    "\n",
    "# we will maintain a record of models trained for different values of lambda\n",
    "# these will be indexed directly by lambda value itself\n",
    "trained_models = dict()\n",
    "\n",
    "# set the (initial?) set of hyperparameters to explore\n",
    "\n",
    "lambda_=0.0001\n",
    "hidden_size_=[1000,1000] # two hidden layers, 1000 units each\n",
    "\n",
    "trainopt['lambda'] = lambda_\n",
    "model = build_model(NFEATURES, hidden_size_, 10, dropout = 0.8)\n",
    "crit = SoftMaxLoss()\n",
    "# -- model trained on large train set\n",
    "trained_model,valErr,trainErr = runTrainVal(Xlarge, Ylarge, model, Xval, Yval, trainopt)\n",
    "trained_models[lambda_] = {'model': trained_model, \"val_err\": valErr, \"train_err\": trainErr }\n",
    "print('train set model [ h = ',end='')\n",
    "for l in range(len(hidden_size_)):\n",
    "    print('%d '%hidden_size_[l],end='')\n",
    "print(' ], lambda= %.4f ] --> train error: %.2f, val error: %.2f' % (lambda_, trainErr, valErr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model = trained_models[0.0001]['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: submission-mnist.csv\n"
     ]
    }
   ],
   "source": [
    "#Generate a Kaggle submission file using best_trained_model which you should set based on your experiments\n",
    "kaggleX = FMNIST_utils.load_data('kaggle')\n",
    "kaggleYhat = predict(kaggleX, best_model).argmax(-1)\n",
    "save_submission('submission-mnist.csv', kaggleYhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments on MNIST data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the MNIST data set, I trained two different models. One with 5 hidden layers and 200 units per layer and the other with only two hidden layers, but 1000 units per layer.\n",
    "\n",
    "I chose the same dropout rate (0.8) for both models, which seems to be the recommended rate for images in the following paper : *Dropout: A Simple Way to Prevent Neural Networks from Overfitting* https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf\n",
    "\n",
    "As the results show, a wider network seems to do a better job than a deeper network in these examples. The best model was the one with two hidden layers and 1000 units per layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consumer Review Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's work on consumer review data (from Pset 3). We will use essentially the same code for the model and training as for Fashion MNIST, but apply it to another data set, with another set of feature functions (mapping text to vectors, instead of mapping images to vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature size:  2104\n"
     ]
    }
   ],
   "source": [
    "import CR_utils\n",
    "X, Y, keys = CR_utils.preprocess(use_bigram = False,mincount=3)\n",
    "X_train = X['train']\n",
    "Y_train = CR_utils.binarize_labels(np.expand_dims(Y['train'],1))\n",
    "X_val = X['val']\n",
    "Y_val = CR_utils.binarize_labels(np.expand_dims(Y['val'],1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again an example of how to train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Model : One layer, 400 units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2104, 400) Weight initial random values\n",
      "(400, 2) Weight initial random values\n",
      "       0 batch loss: 0.710 train error: 41.477 val error: 39.600\n",
      "    1000 batch loss: 0.735 train error: 24.685 val error: 32.000\n",
      "    2000 batch loss: 0.399 train error: 21.189 val error: 27.200\n",
      "    3000 batch loss: 0.113 train error: 14.450 val error: 28.200\n",
      "    4000 batch loss: 0.066 train error: 14.378 val error: 27.000\n",
      "    5000 batch loss: 0.527 train error: 11.928 val error: 27.200\n",
      "    6000 batch loss: 0.117 train error: 8.288 val error: 24.800\n",
      "    7000 batch loss: 0.228 train error: 9.946 val error: 27.600\n",
      "    8000 batch loss: 0.003 train error: 7.820 val error: 25.200\n",
      "    9000 batch loss: 0.181 train error: 5.225 val error: 25.800\n",
      "train set model [ h = 400  ], lambda= 0.0001 ] --> train error: 5.23, val error: 24.80\n"
     ]
    }
   ],
   "source": [
    "# -- training options\n",
    "trainopt = {\n",
    "    'eta': 1e-1,   # initial learning rate\n",
    "    'maxiter': 10000,   # max number of iterations (updates) of SGD\n",
    "    'display_iter': 1000,  # display batch loss every display_iter updates\n",
    "    'batch_size': 1,  \n",
    "    'etadrop': .5, # when dropping eta, multiply it by this number (e.g., .5 means halve it)\n",
    "    'eta_frac': .25,  # how ofter to drop eta\n",
    "    'update': 'sgdm'\n",
    "}\n",
    "\n",
    "NFEATURES = len(keys)\n",
    "\n",
    "lambda_=0.0001\n",
    "hidden_size_=[400] # one hidden layer, 400 units\n",
    "\n",
    "trainopt['lambda'] = lambda_\n",
    "model = build_model(NFEATURES, hidden_size_, 2, dropout = 0.2)\n",
    "crit = SoftMaxLoss()\n",
    "# -- model trained on large train set\n",
    "trained_model,valErr,trainErr = runTrainVal(X_train, Y_train, model, X_val, Y_val, trainopt)\n",
    "trained_models[lambda_] = {'model': trained_model, \"val_err\": valErr, \"train_err\": trainErr }\n",
    "print('train set model [ h = ',end='')\n",
    "for l in range(len(hidden_size_)):\n",
    "    print('%d '%hidden_size_[l],end='')\n",
    "print(' ], lambda= %.4f ] --> train error: %.2f, val error: %.2f' % (lambda_, trainErr, valErr))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Training: 3 hidden layers, 800 units per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2104, 800) Weight initial random values\n",
      "(800, 800) Weight initial random values\n",
      "(800, 800) Weight initial random values\n",
      "(800, 2) Weight initial random values\n",
      "       0 batch loss: 0.701 train error: 36.360 val error: 33.000\n",
      "    1000 batch loss: 0.882 train error: 33.802 val error: 31.200\n",
      "    2000 batch loss: 0.383 train error: 26.631 val error: 33.400\n",
      "    3000 batch loss: 0.606 train error: 18.811 val error: 28.800\n",
      "    4000 batch loss: 0.488 train error: 20.396 val error: 29.000\n",
      "    5000 batch loss: 0.020 train error: 15.063 val error: 26.600\n",
      "    6000 batch loss: 0.606 train error: 11.207 val error: 27.600\n",
      "    7000 batch loss: 0.069 train error: 9.838 val error: 27.000\n",
      "    8000 batch loss: 0.073 train error: 6.739 val error: 27.400\n",
      "    9000 batch loss: 0.155 train error: 5.225 val error: 26.200\n",
      "train set model [ h = 800 800 800  ], lambda= 0.0001 ] --> train error: 5.23, val error: 26.20\n"
     ]
    }
   ],
   "source": [
    "# -- training options\n",
    "trainopt = {\n",
    "    'eta': 1e-1,   # initial learning rate\n",
    "    'maxiter': 10000,   # max number of iterations (updates) of SGD\n",
    "    'display_iter': 1000,  # display batch loss every display_iter updates\n",
    "    'batch_size': 1,  \n",
    "    'etadrop': .5, # when dropping eta, multiply it by this number (e.g., .5 means halve it)\n",
    "    'eta_frac': .25,  # how ofter to drop eta\n",
    "    'update': 'sgdm'\n",
    "}\n",
    "\n",
    "NFEATURES = len(keys)\n",
    "\n",
    "lambda_=0.0001\n",
    "hidden_size_=[800, 800, 800] # one hidden layer, 400 units\n",
    "\n",
    "trainopt['lambda'] = lambda_\n",
    "model = build_model(NFEATURES, hidden_size_, 2, dropout = 0.2)\n",
    "crit = SoftMaxLoss()\n",
    "# -- model trained on large train set\n",
    "trained_model,valErr,trainErr = runTrainVal(X_train, Y_train, model, X_val, Y_val, trainopt)\n",
    "trained_models[lambda_] = {'model': trained_model, \"val_err\": valErr, \"train_err\": trainErr }\n",
    "print('train set model [ h = ',end='')\n",
    "for l in range(len(hidden_size_)):\n",
    "    print('%d '%hidden_size_[l],end='')\n",
    "print(' ], lambda= %.4f ] --> train error: %.2f, val error: %.2f' % (lambda_, trainErr, valErr))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model_cr = trained_models[0.0001]['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_hat = predict(X['test'], best_model_cr).argmax(-1) * 2 - 1\n",
    "CR_utils.save_submission('submission-CR.csv', y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments on Consumer Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the consumer data set, I decided to train two models: \n",
    " - The model with the default parameters provided in the assignment. \n",
    " - Using what I learned from training the MNIST data, I increased the units to 800 and added to layers.\n",
    "While the model with the default paremeters seems to have a smaller error on the validation set, our intuition is that a more complex model (complexity in terms of more units and layers) might be better to learn from the features in the data set.\n",
    "\n",
    "After sending both predictions to Kaggle, it seems that the more complex model had better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
